# UT Campus Object Dataset (CODa) Object Detection Models

<b>Official model development kit for CODa.</b> We strongly recommend using this repository to run our pretrained
models and train on custom datasets. Thanks to the authors of ST3D++ and OpenPCDet from whom this repository
was adapted from.

![Sequence 0 Clip](./docs/codademo.gif)

## Installation

Please refer to [INSTALL.md](docs/INSTALL.md) for the installation.

## Getting Started

### Quicksetup with Docker

Please refer to [DOCKER.md](docs/DOCKER.md) to learn about how to pull our prebuilt docker images to **deploy our pretrained 3D object detection models.**

### Full Setup
Please refer to [GETTING_STARTED.md](docs/GETTING_STARTED.md) to learn more about how to use this project.

## License

Our code is released under the Apache 2.0 license.

## Paper Citation

If you find our work useful in your research, please consider citing our [paper](https://arxiv.org/abs/2309.13549) and [dataset](https://dataverse.tdl.org/dataset.xhtml?persistentId=doi:10.18738/T8/BBOQMV):

```
@inproceedings{zhang2023utcoda,
    title={Towards Robust 3D Robot Perception in Urban Environments: The UT Campus Object Dataset},
    author={Arthur Zhang and Chaitanya Eranki and Christina Zhang and Raymond Hong and Pranav Kalyani and Lochana Kalyanaraman and Arsh Gamare and Maria Esteva and Joydeep Biswas },
    booktitle={},
    year={2023}
}
```

## Dataset Citation
```
@data{T8/BBOQMV_2023,
author = {Zhang, Arthur and Eranki, Chaitanya and Zhang, Christina and Hong, Raymond and Kalyani, Pranav and Kalyanaraman, Lochana and Gamare, Arsh and Bagad, Arnav and Esteva, Maria and Biswas, Joydeep},
publisher = {Texas Data Repository},
title = {{UT Campus Object Dataset (CODa)}},
year = {2023},
version = {DRAFT VERSION},
doi = {10.18738/T8/BBOQMV},
url = {https://doi.org/10.18738/T8/BBOQMV}
}
```

## Acknowledgement

Our code is heavily based on [OpenPCDet v0.3](https://github.com/open-mmlab/OpenPCDet/commit/e3bec15f1052b4827d942398f20f2db1cb681c01). Thanks OpenPCDet Development Team for their awesome codebase.


Thank you to the authors of ST3D++ or OpenPCDet for an awesome codebase!
```
@article{yang2021st3d++,
  title={ST3D++: Denoised Self-training for Unsupervised Domain Adaptation on 3D Object Detection},
  author={Yang, Jihan and Shi, Shaoshuai and Wang, Zhe and Li, Hongsheng and Qi, Xiaojuan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022}
}
```
```
@misc{openpcdet2020,
    title={OpenPCDet: An Open-source Toolbox for 3D Object Detection from Point Clouds},
    author={OpenPCDet Development Team},
    howpublished = {\url{https://github.com/open-mmlab/OpenPCDet}},
    year={2020}
}
```

---

# CODa Dataset í•™ìŠµ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [PKL íŒŒì¼ ê°œë…](#pkl-íŒŒì¼-ê°œë…)
2. [ë°œìƒí•œ ë¬¸ì œë“¤](#ë°œìƒí•œ-ë¬¸ì œë“¤)
3. [í•´ê²° ê³¼ì •](#í•´ê²°-ê³¼ì •)
4. [ë…¼ë¬¸/GitHub ì„¤ì • ë¹„êµ](#ë…¼ë¬¸github-ì„¤ì •-ë¹„êµ)
5. [í•™ìŠµ ëª…ë ¹ì–´](#í•™ìŠµ-ëª…ë ¹ì–´)
6. [í‰ê°€ ëª…ë ¹ì–´](#í‰ê°€-ëª…ë ¹ì–´)
7. [ë‹¤ë¥¸ ë°ì´í„°ì…‹ ì ìš© ì‹œ ì²´í¬ë¦¬ìŠ¤íŠ¸](#ë‹¤ë¥¸-ë°ì´í„°ì…‹-ì ìš©-ì‹œ-ì²´í¬ë¦¬ìŠ¤íŠ¸)

---

## ğŸ” PKL íŒŒì¼ ê°œë…

### PKL (Pickle) íŒŒì¼ì´ë€?
- **ì—­í• **: ë°ì´í„°ì…‹ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì²˜ë¦¬í•˜ì—¬ ì €ì¥
- **ëª©ì **: í•™ìŠµ ì‹œ ë§¤ë²ˆ ì›ë³¸ ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ì§€ ì•Šê³  ë¹ ë¥´ê²Œ ë¡œë“œ

### PKL íŒŒì¼ êµ¬ì¡°
```
coda_infos_train.pkl
â”œâ”€â”€ frame_id: "0501900"
â”œâ”€â”€ image: {"image_path": "...", "image_shape": [...]}
â”œâ”€â”€ point_cloud: {"lidar_path": "..."}
â”œâ”€â”€ calib: {P0, P1, P2, P3, R0_rect, Tr_velo_to_cam}
â””â”€â”€ annos:
    â”œâ”€â”€ name: ["Pedestrian", "Pole", ...]
    â”œâ”€â”€ bbox: [[x1,y1,x2,y2], ...]  # 2D bbox
    â”œâ”€â”€ location: [[x,y,z], ...]     # 3D center
    â”œâ”€â”€ dimensions: [[h,w,l], ...]   # 3D size
    â”œâ”€â”€ rotation_y: [...]            # Rotation
    â”œâ”€â”€ score: [...]
    â”œâ”€â”€ difficulty: [...]
    â””â”€â”€ num_points_in_gt: [...]
```

### ì™œ í•„ìš”í•œê°€?
1. **ì†ë„**: ì›ë³¸ JSON íŒŒì‹± ëŒ€ì‹  ì§ë ¬í™”ëœ ë°ì´í„° ë¡œë“œ (10-100ë°° ë¹ ë¦„)
2. **ì „ì²˜ë¦¬**: ì¢Œí‘œ ë³€í™˜, í•„í„°ë§ ë“±ì„ ë¯¸ë¦¬ ìˆ˜í–‰
3. **ì¼ê´€ì„±**: ëª¨ë“  í”„ë ˆì„ì´ ë™ì¼í•œ í¬ë§·ìœ¼ë¡œ ì €ì¥
4. **ë°ì´í„° ì¦ê°•**: GT databaseë¥¼ ë¯¸ë¦¬ ìƒì„±í•˜ì—¬ augmentationì— ì‚¬ìš©

---

## âŒ ë°œìƒí•œ ë¬¸ì œë“¤

### 1. ë°ì´í„° ë³€í™˜ ë¬¸ì œ
#### ë¬¸ì œ 1-1: ì´ë¯¸ì§€ í™•ì¥ì ë¶ˆì¼ì¹˜
- **ì¦ìƒ**: `AssertionError: Image file does not exist: .../2d_rect_cam0_5_1900.jpg`
- **ì›ì¸**: CODaëŠ” `.png`ì¸ë° converterëŠ” `.jpg` ì˜ˆìƒ
- **ìœ„ì¹˜**: `tools/data_converter/coda_converter.py:288`

#### ë¬¸ì œ 1-2: 2D bbox íŒŒì¼ ì—†ìŒ
- **ì¦ìƒ**: `FileNotFoundError: .../2d_bbox/cam0/5/2d_bbox_cam0_5_1953.txt not found`
- **ì›ì¸**: CODaëŠ” LiDAR ì „ìš© ë°ì´í„°ì…‹, 2D bbox ë¯¸ì œê³µ
- **ìœ„ì¹˜**: `tools/data_converter/coda_converter.py:477-520`

#### ë¬¸ì œ 1-3: 3D ë°ì´í„° ê²½ë¡œ ë¶ˆì¼ì¹˜
- **ì¦ìƒ**: `AssertionError: Bin file does not exist: .../3d_raw/os1/5/...`
- **ì›ì¸**: CODaëŠ” `3d_comp` ì‚¬ìš©, converterëŠ” `3d_raw` ì°¾ìŒ
- **ìœ„ì¹˜**: `tools/data_converter/coda_converter.py:291, 443-444`

### 2. PKL ìƒì„± ë¬¸ì œ
#### ë¬¸ì œ 2-1: Split íŒŒë¼ë¯¸í„° ë²„ê·¸ âš ï¸ **Critical**
- **ì¦ìƒ**: `Total samples for CODa dataset: 0`
- **ì›ì¸**: `coda_dataset.py:136`ì—ì„œ `self.split` ëŒ€ì‹  `split` íŒŒë¼ë¯¸í„° ì‚¬ìš©í•´ì•¼ í•¨
```python
# WRONG:
split_dir = self.root_path / 'ImageSets' / (self.split + '.txt')

# CORRECT:
split_dir = self.root_path / 'ImageSets' / (split + '.txt')
```

#### ë¬¸ì œ 2-2: ì˜ëª»ëœ PKL ì¬ì‚¬ìš©
- **ì¦ìƒ**: `AssertionError: Lidar files data/.../0102019.bin`
- **ì›ì¸**: ë‹¤ë¥¸ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì˜ PKL ë³µì‚¬ (3,496 ìƒ˜í”Œ vs 140 ìƒ˜í”Œ, ë‹¤ë¥¸ sample ID)
- **í•´ê²°**: ì²˜ìŒë¶€í„° ì¬ìƒì„± í•„ìš”

#### ë¬¸ì œ 2-3: Python ëª¨ë“ˆ ìºì‹±
- **ì¦ìƒ**: ì½”ë“œ ìˆ˜ì • í›„ì—ë„ ë²„ê·¸ ì§€ì†
- **ì›ì¸**: Pythonì´ ì´ë¯¸ importëœ ëª¨ë“ˆ ìºì‹œ
- **í•´ê²°**: `python -B -m pcdet.datasets.coda.coda_dataset` (bytecode ìºì‹± ë¹„í™œì„±í™”)

### 3. í•™ìŠµ ì„¤ì • ë¬¸ì œ
#### ë¬¸ì œ 3-1: WandB ì´ˆê¸°í™” ì˜¤ë¥˜
- **ì¦ìƒ**: `wandb.errors.errors.UsageError: api_key not configured`
- **ì›ì¸**: configì—ì„œ `WANDB: False`ì¸ë°ë„ `wandb.init()` í˜¸ì¶œ
- **ìœ„ì¹˜**: `tools/train_utils/train_utils.py:72, 42` / `tools/train.py:313`

#### ë¬¸ì œ 3-2: Distributed training íŒŒë¼ë¯¸í„°
- **ì¦ìƒ**: `RuntimeError: Default process group has not been initialized`
- **ì›ì¸**: `torchrun`ì´ `LOCAL_RANK` í™˜ê²½ë³€ìˆ˜ë¡œ ì „ë‹¬í•˜ëŠ”ë° ì½”ë“œëŠ” `--local-rank` íŒŒë¼ë¯¸í„° ì˜ˆìƒ
- **í•´ê²°**: í™˜ê²½ë³€ìˆ˜ ì§€ì› ì¶”ê°€ (`tools/train.py:53-57`)

### 4. í´ë˜ìŠ¤ ì´ë¦„ ë¬¸ì œ
#### ë¬¸ì œ 4-1: Car vs UtilityVehicle
- **ì¦ìƒ**: Car AP 0%
- **ì›ì¸**: Sequence 5ì—ëŠ” `UtilityVehicle`ë§Œ ìˆê³  `Car` ì—†ìŒ
- **í•´ê²°**:
  - ì˜µì…˜ 1: í˜„ì¬ ëª¨ë¸ ì‚¬ìš© (Pedestrian, Cyclistë§Œ ê²€ì¶œ)
  - ì˜µì…˜ 2: configë¥¼ `UtilityVehicle`ë¡œ ë³€ê²½ í›„ ì¬í•™ìŠµ

---

## âœ… í•´ê²° ê³¼ì •

### Step 0: í™˜ê²½ ì¤€ë¹„
```bash
# ì‘ì—… ë””ë ‰í† ë¦¬
cd /media/withsu/ROBOT_SSD_0/coda_clone2/coda-models

# ì›ë³¸ ë°ì´í„° ìœ„ì¹˜ í™•ì¸
ls /media/withsu/ROBOT_SSD_0/5/
# ì¶œë ¥: 2d_rect  3d_bbox  3d_comp  calibrations  metadata  poses  timestamps
```

### Step 1: ë°ì´í„° ë³€í™˜ (CODa â†’ KITTI Format)

#### 1-1. Converter ìˆ˜ì •
```bash
# íŒŒì¼: tools/data_converter/coda_converter.py
```

**ìˆ˜ì • 1: ì´ë¯¸ì§€ í™•ì¥ì** (Line 288)
```python
if "2d_rect"==modality:
    filetype = "png"  # ì›ë˜: "jpg"
```

**ìˆ˜ì • 2: 2D bbox ë”ë¯¸ê°’** (Lines 477-520)
```python
# CODaëŠ” 2D bbox ì—†ìŒ - ë”ë¯¸ê°’ ì‚¬ìš©
twod_anno_dict = None
# ... ë‚˜ì¤‘ì— ...
bounding_box = [0.0, 0.0, 50.0, 50.0]  # ë”ë¯¸ê°’ (LiDARë§Œ ì‚¬ìš©)
```

**ìˆ˜ì • 3: 3d_comp ê²½ë¡œ** (Lines 291, 443-444)
```python
elif "3d_raw"==modality or "3d_comp"==modality:  # ì›ë˜: "3d_raw"ë§Œ
    filetype = "bin"

# ...
bin_file = self.set_filename_by_prefix("3d_comp", "os1", traj, frame_idx)  # ì›ë˜: "3d_raw"
bin_path = join(self.load_dir, "3d_comp", "os1", traj, bin_file)
```

#### 1-2. ë³€í™˜ ì‹¤í–‰
```bash
cd /media/withsu/ROBOT_SSD_0/coda_clone2/coda-models

export PYTHONPATH=$PWD:$PYTHONPATH

python tools/create_data.py coda \
  --root-path ./ \
  --out-dir ./data \
  --workers 8 \
  2>&1 | tee conversion.log
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Processing trajectory 5...
Converting frame 1800/2000...
...
Conversion complete: 200 frames
```

**ê²°ê³¼ í™•ì¸:**
```bash
ls data/coda_kitti_format/
# ì¶œë ¥: ImageSets  training  testing

ls data/coda_kitti_format/ImageSets/
# ì¶œë ¥: train.txt  val.txt  test.txt

wc -l data/coda_kitti_format/ImageSets/*.txt
# ì¶œë ¥:
#  140 train.txt
#   30 val.txt
#   30 test.txt
```

### Step 2: PKL íŒŒì¼ ìƒì„±

#### 2-1. Dataset ì½”ë“œ ìˆ˜ì •
```bash
# íŒŒì¼: pcdet/datasets/coda/coda_dataset.py
```

**ìˆ˜ì • 1: Split íŒŒë¼ë¯¸í„°** (Line 136) âš ï¸ **ê°€ì¥ ì¤‘ìš”!**
```python
def set_sample_id_list(self, split):
    split_dir = self.root_path / 'ImageSets' / (split + '.txt')  # ì›ë˜: self.split
```

**ìˆ˜ì • 2: ì´ë¯¸ì§€ í™•ì¥ì** (Line 162)
```python
img_file = root_split_path / 'image_0' / ('%s.png' % idx)  # ì›ë˜: .jpg
```

**ìˆ˜ì • 3: Data path** (Lines 671-672)
```python
data_path=ROOT_DIR / 'data' / 'coda_kitti_format',  # ì›ë˜: coda128_allclass_full
save_path=ROOT_DIR / 'data' / 'coda_kitti_format',
```

#### 2-2. Config ìˆ˜ì •
```bash
# íŒŒì¼: tools/cfgs/dataset_configs/da_coda_oracle_dataset_3class.yaml
```

```yaml
DATA_PATH: 'data/coda_kitti_format'  # ì›ë˜: '../data/coda_kitti_format'
```

#### 2-3. PKL ìƒì„± ì‹¤í–‰
```bash
# -B í”Œë˜ê·¸ë¡œ Python bytecode ìºì‹± ë¹„í™œì„±í™” (ì¤‘ìš”!)
python -B -m pcdet.datasets.coda.coda_dataset \
  create_coda_infos \
  tools/cfgs/dataset_configs/da_coda_oracle_dataset_3class.yaml \
  2>&1 | tee pkl_generation.log
```

**ì˜ˆìƒ ì¶œë ¥:**
```
---------------Start to generate data infos---------------
train sample_idx: 0501900
train sample_idx: 0501942
...
CODa info train file is saved to .../coda_infos_train.pkl
CODa info val file is saved to .../coda_infos_val.pkl
CODa info test file is saved to .../coda_infos_test.pkl
---------------Start create groundtruth database for data augmentation---------------
Database Pole: 1417
Database Pedestrian: 968
Database Tree: 971
Database Cyclist: 149
Database Railing: 1234
Database BikeRack: 316
Database UtilityVehicle: 25
---------------Data preparation Done---------------
```

**ê²°ê³¼ í™•ì¸:**
```bash
ls -lh data/coda_kitti_format/*.pkl
# ì¶œë ¥:
# -rw-r--r-- 1.2M coda_infos_train.pkl      (140 samples)
# -rw-r--r-- 243K coda_infos_val.pkl        (30 samples)
# -rw-r--r-- 241K coda_infos_test.pkl       (30 samples)
# -rw-r--r-- 1.5M coda_dbinfos_train.pkl    (GT database)
```

### Step 3: í•™ìŠµ ì„¤ì •

#### 3-1. Config ìˆ˜ì •
```bash
# íŒŒì¼: tools/cfgs/coda_models/pointpillar_1x.yaml
```

**ìˆ˜ì • 1: Base config ê²½ë¡œ** (Line 4)
```yaml
DATA_CONFIG:
    _BASE_CONFIG_: tools/cfgs/dataset_configs/da_coda_oracle_dataset_3class.yaml
```

**ìˆ˜ì • 2: WandB ë¹„í™œì„±í™”** (Line 129)
```yaml
FINETUNE:
    WANDB: False
```

**ìˆ˜ì • 3: Balanced resampling** (Line 5)
```yaml
DATA_CONFIG:
    BALANCED_RESAMPLING: False  # ì›ë˜: True (ì—ëŸ¬ ë°œìƒ)
```

#### 3-2. Train ì½”ë“œ ìˆ˜ì •
```bash
# íŒŒì¼: tools/train.py
```

**ìˆ˜ì • 1: LOCAL_RANK í™˜ê²½ë³€ìˆ˜ ì§€ì›** (Lines 53-57)
```python
args = parser.parse_args()

# torchrunì´ í™˜ê²½ë³€ìˆ˜ë¡œ ì „ë‹¬
if 'LOCAL_RANK' in os.environ:
    args.local_rank = int(os.environ['LOCAL_RANK'])

cfg_from_yaml_file(args.cfg_file, cfg)
```

**ìˆ˜ì • 2: WandB ì²´í¬** (Line 313)
```python
if ft_cfg is not None and ft_cfg.get('WANDB', False):  # ì¶”ê°€: WANDB ì²´í¬
    wandb.init(...)
```

#### 3-3. Train utils ìˆ˜ì •
```bash
# íŒŒì¼: tools/train_utils/train_utils.py
```

**ìˆ˜ì • 1: WandB ì´ˆê¸°í™” ì²´í¬** (Line 72)
```python
if ft_cfg is not None and ft_cfg.get('WANDB', False):
    wandb.init(...)
```

**ìˆ˜ì • 2: WandB ë¡œê¹… ì²´í¬** (Line 42)
```python
if ft_cfg is not None and ft_cfg.get('WANDB', False):
    wandb.log({"loss": loss, "lr": cur_lr, "iter": cur_it})
```

### Step 4: í•™ìŠµ ì‹¤í–‰

```bash
torchrun --standalone --nnodes=1 --nproc_per_node=1 \
  tools/train.py \
  --cfg_file tools/cfgs/coda_models/pointpillar_1x.yaml \
  --launcher pytorch \
  --batch_size 2 \
  2>&1 | tee training_pointpillar_50epochs.log
```

**í•™ìŠµ íŒŒë¼ë¯¸í„°:**
- Epochs: 50
- Batch size: 2 per GPU
- Learning rate: 0.003
- Optimizer: adam_onecycle
- ì†Œìš” ì‹œê°„: ì•½ 40ë¶„ (RTX 3090 ê¸°ì¤€)

**ê²°ê³¼:**
```
Epoch 50/50: loss=0.458
Checkpoints saved to: output/.../ckpt/checkpoint_epoch_50.pth
```

### Step 5: í‰ê°€ ì‹¤í–‰

#### 5-1. í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
```bash
# íŒŒì¼: eval_model.py (ìƒˆë¡œ ìƒì„±)
```

```python
#!/usr/bin/env python3
"""Simple evaluation script for trained models"""
import sys, os, torch, argparse
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent / 'tools'))

from pcdet.datasets import build_dataloader
from pcdet.models import build_network
from pcdet.utils import common_utils
from pcdet.config import cfg, cfg_from_yaml_file
from eval_utils import eval_utils

def parse_args():
    parser = argparse.ArgumentParser(description='Evaluate trained model')
    parser.add_argument('--cfg_file', type=str, required=True)
    parser.add_argument('--ckpt', type=str, required=True)
    parser.add_argument('--batch_size', type=int, default=1)
    parser.add_argument('--workers', type=int, default=4)
    parser.add_argument('--split', type=str, default='val', choices=['train', 'val', 'test'])
    return parser.parse_args()

def main():
    args = parse_args()
    cfg_from_yaml_file(args.cfg_file, cfg)
    cfg.TAG = Path(args.cfg_file).stem
    cfg.EXP_GROUP_PATH = '/'.join(args.cfg_file.split('/')[1:-1])
    cfg.DATA_CONFIG.DATA_SPLIT['test'] = args.split

    output_dir = Path('output') / cfg.EXP_GROUP_PATH / cfg.TAG / 'eval' / f'eval_{args.split}'
    output_dir.mkdir(parents=True, exist_ok=True)

    log_file = output_dir / 'eval_log.txt'
    logger = common_utils.create_logger(log_file, rank=0)

    test_set, test_loader, sampler = build_dataloader(
        dataset_cfg=cfg.DATA_CONFIG, class_names=cfg.CLASS_NAMES,
        batch_size=args.batch_size, dist=False, workers=args.workers,
        logger=logger, training=False
    )

    model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), dataset=test_set)
    model.load_params_from_file(filename=args.ckpt, logger=logger, to_cpu=False)
    model.cuda()
    model.eval()

    with torch.no_grad():
        eval_utils.eval_one_epoch(
            cfg, model, test_loader, 0, logger,
            dist_test=False, result_dir=output_dir, save_to_file=True
        )

if __name__ == '__main__':
    main()
```

#### 5-2. í‰ê°€ ì‹¤í–‰
```bash
python eval_model.py \
  --cfg_file tools/cfgs/coda_models/pointpillar_1x.yaml \
  --ckpt output/cfgs/coda_models/pointpillar_1x/defaultLR0.003000OPTadam_onecycle/ckpt/checkpoint_epoch_50.pth \
  --split val \
  --batch_size 1 \
  2>&1 | tee evaluation_val_epoch50.log
```

**ê²°ê³¼ í™•ì¸:**
```bash
# ë¡œê·¸ íŒŒì¼ì—ì„œ AP í™•ì¸
grep "AP_R40" evaluation_val_epoch50.log

# ë˜ëŠ” ê²°ê³¼ ë””ë ‰í† ë¦¬ í™•ì¸
ls output/cfgs/coda_models/pointpillar_1x/eval/eval_val/
```

**í‰ê°€ ê²°ê³¼ (Epoch 50):**
```
Pedestrian 3D AP_R40@0.50: 85.23% / 85.23% / 88.85% (easy/moderate/hard)
Cyclist 3D AP_R40@0.50:    12.69% / 12.69% / 63.15%
Car 3D AP_R40@0.70:         0.00% /  0.00% /  0.00% (ë°ì´í„° ì—†ìŒ)
```

---

## ğŸ“Š ë…¼ë¬¸/GitHub ì„¤ì • ë¹„êµ

### Voxel Size ë¹„êµ

| êµ¬ë¶„ | X | Y | Z | MAX_VOXELS (train/test) | ë¹„ê³  |
|------|---|---|---|------------------------|------|
| **GitHub ê³µì‹** | 0.1 | 0.1 | 6.0 | 80K / 90K | âœ… ê¶Œì¥ |
| **ì´ì „ ì„¤ì •** | 0.18 | 0.18 | 6.0 | 60K / 70K | âŒ ì˜ëª»ë¨ |
| **í˜„ì¬ ì„¤ì •** | 0.1 | 0.1 | 6.0 | 80K / 90K | âœ… ì˜¬ë°”ë¦„ |

**Voxel Sizeê°€ ì¤‘ìš”í•œ ì´ìœ :**
- ì‘ì€ voxel (0.1) â†’ ë†’ì€ í•´ìƒë„, ë” ì •í™•í•œ ê²€ì¶œ
- í° voxel (0.18) â†’ ë‚®ì€ í•´ìƒë„, ì‘ì€ ê°ì²´ ëˆ„ë½ ê°€ëŠ¥
- 0.18 ì‚¬ìš© ì‹œ ì„±ëŠ¥ ì €í•˜ ì˜ˆìƒ: 17.94% mAP â†’ 48.86% mAP (ë…¼ë¬¸ ê¸°ì¤€ ì•½ 2.7ë°° ì°¨ì´)

### Point Cloud Range

| êµ¬ë¶„ | Range (m) | ë¹„ê³  |
|------|-----------|------|
| **Config** | [-35, -35, -2] ~ [35, 35, 4] | 70m Ã— 70m |
| **ì‹¤ì œ ë°ì´í„°** | ë” ë„“ì€ ë²”ìœ„ ê°€ëŠ¥ | Ouster OS1-128 |

### í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê°’ | ë¹„ê³  |
|----------|-----|------|
| Epochs | 50 | GitHub ê¸°ë³¸ê°’ |
| Batch Size | 2 per GPU | GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • |
| Learning Rate | 0.003 | adam_onecycle |
| Optimizer | adam_onecycle | - |
| Weight Decay | 0.01 | - |
| Gradient Clip | 10 | - |

### í´ë˜ìŠ¤ ì„¤ì •

#### GitHub ê³µì‹ ì„¤ì •
```yaml
CLASS_NAMES: ['Car', 'Pedestrian', 'Cyclist']
```

#### ì‹¤ì œ ë°ì´í„° (Sequence 5)
- **ìˆëŠ” í´ë˜ìŠ¤**: UtilityVehicle, Pedestrian, Bike (â†’Cyclist), Pole, Tree, Railing, Bike Rack
- **ì—†ëŠ” í´ë˜ìŠ¤**: Car

#### í•´ê²° ë°©ë²•
**ì˜µì…˜ 1**: í˜„ì¬ ëª¨ë¸ ì‚¬ìš© (Pedestrian, Cyclistë§Œ ê²€ì¶œ)
**ì˜µì…˜ 2**: CONFIG ìˆ˜ì •
```yaml
CLASS_NAMES: ['UtilityVehicle', 'Pedestrian', 'Cyclist']

# Anchor configë„ ìˆ˜ì •
ANCHOR_GENERATOR_CONFIG:
    - class_name: 'UtilityVehicle'  # ì›ë˜: 'Vehicle'
```

---

## ğŸš€ í•™ìŠµ ëª…ë ¹ì–´

### ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸

```bash
# 0. ì‘ì—… ë””ë ‰í† ë¦¬ ì´ë™
cd /media/withsu/ROBOT_SSD_0/coda_clone2/coda-models

# 1. í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export PYTHONPATH=$PWD:$PYTHONPATH

# 2. ë°ì´í„° ë³€í™˜ (ìµœì´ˆ 1íšŒë§Œ)
python tools/create_data.py coda \
  --root-path ./ \
  --out-dir ./data \
  --workers 8

# 3. PKL ìƒì„± (ìµœì´ˆ 1íšŒ ë˜ëŠ” ë°ì´í„° ë³€ê²½ ì‹œ)
python -B -m pcdet.datasets.coda.coda_dataset \
  create_coda_infos \
  tools/cfgs/dataset_configs/da_coda_oracle_dataset_3class.yaml

# 4. í•™ìŠµ ì‹¤í–‰
torchrun --standalone --nnodes=1 --nproc_per_node=1 \
  tools/train.py \
  --cfg_file tools/cfgs/coda_models/pointpillar_1x.yaml \
  --launcher pytorch \
  --batch_size 2
```

### Multi-GPU í•™ìŠµ

```bash
# 4ê°œ GPU ì‚¬ìš©
torchrun --standalone --nnodes=1 --nproc_per_node=4 \
  tools/train.py \
  --cfg_file tools/cfgs/coda_models/pointpillar_1x.yaml \
  --launcher pytorch \
  --batch_size 8  # ì´ batch size (GPUë‹¹ 2)
```

### í•™ìŠµ ì¬ê°œ (Resume)

```bash
torchrun --standalone --nnodes=1 --nproc_per_node=1 \
  tools/train.py \
  --cfg_file tools/cfgs/coda_models/pointpillar_1x.yaml \
  --launcher pytorch \
  --batch_size 2 \
  --ckpt output/.../ckpt/checkpoint_epoch_30.pth  # ì¬ê°œí•  checkpoint
```

---

## ğŸ“ˆ í‰ê°€ ëª…ë ¹ì–´

### Validation Set í‰ê°€

```bash
python eval_model.py \
  --cfg_file tools/cfgs/coda_models/pointpillar_1x.yaml \
  --ckpt output/cfgs/coda_models/pointpillar_1x/defaultLR0.003000OPTadam_onecycle/ckpt/checkpoint_epoch_50.pth \
  --split val \
  --batch_size 1
```

### Test Set í‰ê°€

```bash
python eval_model.py \
  --cfg_file tools/cfgs/coda_models/pointpillar_1x.yaml \
  --ckpt output/.../checkpoint_epoch_50.pth \
  --split test \
  --batch_size 1
```

### ëª¨ë“  Checkpoint í‰ê°€

```bash
# output/.../ckpt/ ì•„ë˜ì˜ ëª¨ë“  checkpoint í‰ê°€
for ckpt in output/.../ckpt/checkpoint_epoch_*.pth; do
  epoch=$(basename $ckpt | grep -oP '\d+')
  echo "Evaluating epoch $epoch..."
  python eval_model.py \
    --cfg_file tools/cfgs/coda_models/pointpillar_1x.yaml \
    --ckpt $ckpt \
    --split val
done
```

### ê²°ê³¼ í™•ì¸

```bash
# ë¡œê·¸ íŒŒì¼ì—ì„œ AP í™•ì¸
grep "AP_R40" evaluation_val_epoch50.log

# ë˜ëŠ” ê°„ë‹¨íˆ
grep -A3 "Pedestrian AP_R40" evaluation_val_epoch50.log
grep -A3 "Cyclist AP_R40" evaluation_val_epoch50.log
grep -A3 "Car AP_R40" evaluation_val_epoch50.log
```

---

## âœ… ë‹¤ë¥¸ ë°ì´í„°ì…‹ ì ìš© ì‹œ ì²´í¬ë¦¬ìŠ¤íŠ¸

### 1. ë°ì´í„° í¬ë§· í™•ì¸
- [ ] **ì´ë¯¸ì§€ í™•ì¥ì**: `.jpg` or `.png`?
- [ ] **LiDAR í¬ë§·**: `.bin` (KITTI), `.pcd`, or `.ply`?
- [ ] **3D bbox í¬ë§·**: JSON, TXT, or XML?
- [ ] **ì¢Œí‘œê³„**: Camera or LiDAR ê¸°ì¤€?

### 2. í´ë˜ìŠ¤ í™•ì¸
```bash
# ì‹¤ì œ ë°ì´í„°ì˜ í´ë˜ìŠ¤ í™•ì¸
find <data_path> -name "*.json" | xargs cat | grep '"classId"' | cut -d'"' -f4 | sort -u

# ë˜ëŠ” ë³€í™˜ í›„
cat data/kitti_format/training/label_all/*.txt | cut -d' ' -f1 | sort -u
```

- [ ] **í´ë˜ìŠ¤ ì´ë¦„**: Configì˜ `CLASS_NAMES`ì™€ ì¼ì¹˜?
- [ ] **í´ë˜ìŠ¤ ë§¤í•‘**: Converterì˜ `class_map`ì— ëª¨ë‘ ì •ì˜?

### 3. Converter ìˆ˜ì • ì²´í¬ë¦¬ìŠ¤íŠ¸

**íŒŒì¼**: `tools/data_converter/<dataset>_converter.py`

- [ ] **ì´ë¯¸ì§€ í™•ì¥ì** ìˆ˜ì •
```python
if "2d_rect"==modality:
    filetype = "png"  # ë˜ëŠ” "jpg"
```

- [ ] **LiDAR ê²½ë¡œ** í™•ì¸
```python
bin_path = join(self.load_dir, "3d_comp", ...)  # ì‹¤ì œ ê²½ë¡œë¡œ
```

- [ ] **í´ë˜ìŠ¤ ë§¤í•‘** ì¶”ê°€/ìˆ˜ì •
```python
self.dataset_to_kitti_class_map = {
    'YOUR_CLASS_1': 'Car',
    'YOUR_CLASS_2': 'Pedestrian',
    ...
}
```

- [ ] **2D bbox ì²˜ë¦¬** (ì—†ìœ¼ë©´ ë”ë¯¸ê°’)
```python
bounding_box = [0.0, 0.0, 50.0, 50.0]  # LiDARë§Œ ì‚¬ìš© ì‹œ
```

### 4. Dataset ì½”ë“œ ìˆ˜ì • ì²´í¬ë¦¬ìŠ¤íŠ¸

**íŒŒì¼**: `pcdet/datasets/<dataset>/<dataset>_dataset.py`

- [ ] **Split íŒŒë¼ë¯¸í„° ë²„ê·¸** í™•ì¸
```python
split_dir = self.root_path / 'ImageSets' / (split + '.txt')  # NOT self.split!
```

- [ ] **ì´ë¯¸ì§€ í™•ì¥ì**
```python
img_file = root_split_path / 'image_0' / ('%s.png' % idx)
```

- [ ] **Data path**
```python
data_path=ROOT_DIR / 'data' / '<your_dataset>',
```

### 5. Config ìˆ˜ì • ì²´í¬ë¦¬ìŠ¤íŠ¸

**íŒŒì¼**: `tools/cfgs/<dataset>_models/<model>.yaml`

- [ ] **í´ë˜ìŠ¤ ì´ë¦„** í™•ì¸
```yaml
CLASS_NAMES: ['Car', 'Pedestrian', 'Cyclist']  # ì‹¤ì œ í´ë˜ìŠ¤ë¡œ
```

- [ ] **Anchor ì„¤ì •** (í´ë˜ìŠ¤ë³„)
```yaml
ANCHOR_GENERATOR_CONFIG:
    - class_name: 'Car'  # CLASS_NAMESì™€ ì¼ì¹˜í•´ì•¼ í•¨!
      anchor_sizes: [[4.7, 2.1, 1.7]]  # ì‹¤ì œ ê°ì²´ í¬ê¸° ê¸°ë°˜
```

- [ ] **Point Cloud Range**
```yaml
POINT_CLOUD_RANGE: [-35.0, -35.0, -2.0, 35.0, 35.0, 4.0]  # ë°ì´í„° ë²”ìœ„ì— ë§ê²Œ
```

- [ ] **Voxel Size** (ì¤‘ìš”!)
```yaml
VOXEL_SIZE: [0.1, 0.1, 6.0]  # ì‘ì„ìˆ˜ë¡ ì •ë°€, ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©
MAX_NUMBER_OF_VOXELS: {'train': 80000, 'test': 90000}
```

### 6. PKL ìƒì„± ì „ ì²´í¬

- [ ] **Python ìºì‹œ ë¹„í™œì„±í™”**: `python -B` ì‚¬ìš©
- [ ] **ì´ì „ PKL ì‚­ì œ**: `rm data/<dataset>/coda_infos_*.pkl`
- [ ] **ImageSets í™•ì¸**: `train.txt`, `val.txt`, `test.txt` ì¡´ì¬?
- [ ] **ìƒ˜í”Œ ID í™•ì¸**: ImageSetsì˜ IDì™€ ì‹¤ì œ íŒŒì¼ëª… ì¼ì¹˜?

### 7. í•™ìŠµ ì „ ì²´í¬

- [ ] **GPU ë©”ëª¨ë¦¬**: Batch size ì¡°ì • (2, 4, 8, ...)
- [ ] **Epochs**: ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¼ (50-80 ê¶Œì¥)
- [ ] **Learning Rate**: 0.001-0.003 (batch sizeì— ë¹„ë¡€)
- [ ] **WandB**: ì‚¬ìš© ì—¬ë¶€ ì„¤ì •

### 8. ë””ë²„ê¹… íŒ

#### PKL ìƒì„± ì‹¤íŒ¨ ì‹œ
```bash
# 1. ìƒ˜í”Œ ìˆ˜ í™•ì¸
python -c "
from pcdet.datasets.coda.coda_dataset import CODataset
from pcdet.config import cfg_from_yaml_file, cfg
cfg_from_yaml_file('tools/cfgs/dataset_configs/<config>.yaml', cfg)
dataset = CODataset(cfg.DATA_CONFIG, training=True, logger=None)
print(f'Total samples: {len(dataset)}')
"

# 2. ì²« ìƒ˜í”Œ ë¡œë“œ í…ŒìŠ¤íŠ¸
python -c "
from pcdet.datasets.coda.coda_dataset import CODataset
...
dataset = CODataset(...)
print(dataset[0])
"
```

#### í•™ìŠµ ì‹œì‘ ì‹¤íŒ¨ ì‹œ
```bash
# 1. Config ê²€ì¦
python tools/train.py --cfg_file <config> --launcher none --epochs 1 --batch_size 1

# 2. ë‹¨ì¼ ë°°ì¹˜ ì˜¤ë²„í”¼íŒ… í…ŒìŠ¤íŠ¸
# Configì—ì„œ ì¼ì‹œì ìœ¼ë¡œ:
# OPTIMIZATION.NUM_EPOCHS: 1
# DATA_PROCESSORì—ì„œ ìƒ˜í”Œ 1ê°œë§Œ ì‚¬ìš©
```

#### í‰ê°€ ì‹¤íŒ¨ ì‹œ
```bash
# 1. Checkpoint ë¡œë“œ í™•ì¸
python -c "
import torch
ckpt = torch.load('<checkpoint>.pth')
print('Keys:', ckpt.keys())
print('Epoch:', ckpt['epoch'])
print('Model keys:', list(ckpt['model_state'].keys())[:5])
"

# 2. ë‹¨ì¼ ìƒ˜í”Œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
python eval_model.py ... --batch_size 1 --split val
```

---

## ğŸ“ ì£¼ìš” íŒŒì¼ ê²½ë¡œ ìš”ì•½

```
coda-models/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ coda_kitti_format/
â”‚       â”œâ”€â”€ ImageSets/
â”‚       â”‚   â”œâ”€â”€ train.txt (140 samples)
â”‚       â”‚   â”œâ”€â”€ val.txt (30 samples)
â”‚       â”‚   â””â”€â”€ test.txt (30 samples)
â”‚       â”œâ”€â”€ training/
â”‚       â”‚   â”œâ”€â”€ velodyne/         # LiDAR ë°ì´í„° (.bin)
â”‚       â”‚   â”œâ”€â”€ label_all/        # 3D bbox labels
â”‚       â”‚   â”œâ”€â”€ image_0/          # ì´ë¯¸ì§€ (ì„ íƒ)
â”‚       â”‚   â””â”€â”€ calib/            # Calibration
â”‚       â”œâ”€â”€ coda_infos_train.pkl    (1.2MB)
â”‚       â”œâ”€â”€ coda_infos_val.pkl      (243KB)
â”‚       â”œâ”€â”€ coda_infos_test.pkl     (241KB)
â”‚       â””â”€â”€ coda_dbinfos_train.pkl  (1.5MB)
â”‚
â”œâ”€â”€ output/
â”‚   â””â”€â”€ cfgs/coda_models/pointpillar_1x/defaultLR0.003000OPTadam_onecycle/
â”‚       â”œâ”€â”€ ckpt/
â”‚       â”‚   â”œâ”€â”€ checkpoint_epoch_1.pth
â”‚       â”‚   â”œâ”€â”€ ...
â”‚       â”‚   â””â”€â”€ checkpoint_epoch_50.pth
â”‚       â”œâ”€â”€ log_train_<timestamp>.txt
â”‚       â””â”€â”€ eval/
â”‚           â””â”€â”€ eval_val/
â”‚               â””â”€â”€ eval_log.txt
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ cfgs/
â”‚   â”‚   â”œâ”€â”€ coda_models/
â”‚   â”‚   â”‚   â””â”€â”€ pointpillar_1x.yaml         # ëª¨ë¸ config
â”‚   â”‚   â””â”€â”€ dataset_configs/
â”‚   â”‚       â””â”€â”€ da_coda_oracle_dataset_3class.yaml  # ë°ì´í„° config
â”‚   â”œâ”€â”€ data_converter/
â”‚   â”‚   â””â”€â”€ coda_converter.py               # ë°ì´í„° ë³€í™˜
â”‚   â”œâ”€â”€ train.py                            # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ train_utils/
â”‚       â””â”€â”€ train_utils.py                  # í•™ìŠµ ìœ í‹¸
â”‚
â”œâ”€â”€ pcdet/datasets/coda/
â”‚   â””â”€â”€ coda_dataset.py                     # Dataset í´ë˜ìŠ¤
â”‚
â””â”€â”€ eval_model.py                           # í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ (ìƒˆë¡œ ìƒì„±)
```

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Q1: "Total samples for CODa dataset: 0"
**A**: `coda_dataset.py:136`ì—ì„œ `split` íŒŒë¼ë¯¸í„° ë²„ê·¸ í™•ì¸
```python
split_dir = self.root_path / 'ImageSets' / (split + '.txt')  # NOT self.split!
```

### Q2: PKL ìˆ˜ì • í›„ì—ë„ ë³€í™” ì—†ìŒ
**A**: Python ëª¨ë“ˆ ìºì‹± ë•Œë¬¸. `-B` í”Œë˜ê·¸ ì‚¬ìš©
```bash
python -B -m pcdet.datasets.coda.coda_dataset create_coda_infos ...
```

### Q3: "RuntimeError: Default process group has not been initialized"
**A**: `LOCAL_RANK` í™˜ê²½ë³€ìˆ˜ ì§€ì› í™•ì¸ (`train.py:53-57`)

### Q4: "wandb.errors.errors.UsageError: api_key not configured"
**A**: WandB ì²´í¬ ëˆ„ë½. ë‹¤ìŒ íŒŒì¼ë“¤ í™•ì¸:
- `train_utils.py:72, 42`
- `train.py:313`

### Q5: Car APê°€ 0%
**A**: ë°ì´í„°ì— Car í´ë˜ìŠ¤ê°€ ì—†ìŒ. UtilityVehicle í™•ì¸ í•„ìš”

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **CODa Dataset**: https://utexas.app.box.com/v/coda-paper
- **OpenPCDet**: https://github.com/open-mmlab/OpenPCDet
- **GitHub Repo**: https://github.com/ut-amrl/coda-models

---

**ì‘ì„±ì¼**: 2025-11-02
**ì‘ì„±ì**: Training Pipeline Documentation
**ë°ì´í„°ì…‹**: CODa (UT Campus Object Dataset)
**ëª¨ë¸**: PointPillar (Oracle, 3-class)

---

## ğŸ“Š ìµœì¢… í•™ìŠµ ê²°ê³¼ (PointPillar, Epoch 50)

### Validation Set í‰ê°€ ê²°ê³¼ (30 samples)

**Pedestrian (ë³´í–‰ì)**

| IoU | 3D AP | BEV AP |
|-----|-------|--------|
| 0.50 | 85.23% | 86.85% |
| 0.25 | 88.13% | 88.13% |

**Cyclist (ìì „ê±°)**

| IoU | 3D AP | BEV AP |
|-----|-------|--------|
| 0.50 | 12.69% | 12.70% |
| 0.25 | 13.32% | 13.32% |

**Car**

| IoU | 3D AP | BEV AP |
|-----|-------|--------|
| 0.50 | 0.00% | 0.00% |
| 0.25 | N/A | N/A |

**ì°¸ê³ ì‚¬í•­:**
- Car í´ë˜ìŠ¤ëŠ” Sequence 5ì— ë°ì´í„°ê°€ ì—†ì–´ 0% AP
- Pedestrian ê²€ì¶œ ì„±ëŠ¥ ìš°ìˆ˜ (85-88%)
- CyclistëŠ” Easy/Moderate ë‚œì´ë„ì—ì„œ ë‚®ì€ ì„±ëŠ¥ (ë°ì´í„° ë¶ˆê· í˜•: Training 968 Pedestrian vs 149 Cyclist)
- í•™ìŠµ ì‹œê°„: ì•½ 40ë¶„ (RTX 3090 1 GPU ê¸°ì¤€)
- í•™ìŠµ íŒŒë¼ë¯¸í„°: Voxel Size [0.1, 0.1, 6.0], Batch Size 2, 50 Epochs
